import streamlit as st
import os
import torch
import torchaudio
import pandas as pd
import difflib
import re
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
device = "cuda" if torch.cuda.is_available() else "cpu"
audio_dir = "audio"

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
processor = AutoProcessor.from_pretrained("tarteel-ai/whisper-base-ar-quran")
model = AutoModelForSpeechSeq2Seq.from_pretrained("tarteel-ai/whisper-base-ar-quran")
model.to(device).eval()

st.title("ğŸ“– Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙ„Ø§ÙˆØ© Ø³ÙˆØ±Ø© Ø§Ù„ÙØ§ØªØ­Ø©")

# Ø§Ù„Ø¢ÙŠØ§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ù…Ø¹ Ø£Ø±Ù‚Ø§Ù…Ù‡Ø§
verse_texts = [
    ("001001", "Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù"),
    ("001002", "Ø§Ù„Ù’Ø­ÙÙ…Ù’Ø¯Ù Ù„ÙÙ„Ù‘ÙÙ‡Ù Ø±ÙØ¨Ù‘Ù Ø§Ù„Ù’Ø¹ÙØ§Ù„ÙÙ…ÙÙŠÙ†Ù"),
    ("001003", "Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù"),
    ("001004", "Ù…ÙØ§Ù„ÙÙƒÙ ÙŠÙÙˆÙ’Ù…Ù Ø§Ù„Ø¯Ù‘ÙÙŠÙ†Ù"),
    ("001005", "Ø¥ÙÙŠÙ‘ÙØ§ÙƒÙ Ù†ÙØ¹Ù’Ø¨ÙØ¯Ù ÙˆÙØ¥ÙÙŠÙ‘ÙØ§ÙƒÙ Ù†ÙØ³Ù’ØªÙØ¹ÙÙŠÙ†Ù"),
    ("001006", "Ø§Ù‡Ù’Ø¯ÙÙ†ÙØ§ Ø§Ù„ØµÙ‘ÙØ±ÙØ§Ø·Ù Ø§Ù„Ù’Ù…ÙØ³Ù’ØªÙÙ‚ÙÙŠÙ…Ù"),
    ("001007", "ØµÙØ±ÙØ§Ø·Ù Ø§Ù„Ù‘ÙØ°ÙÙŠÙ†Ù Ø£ÙÙ†Ù’Ø¹ÙÙ…Ù’ØªÙ Ø¹ÙÙ„ÙÙŠÙ’Ù‡ÙÙ…Ù’ ØºÙÙŠÙ’Ø±Ù Ø§Ù„Ù’Ù…ÙØºÙ’Ø¶ÙÙˆØ¨Ù Ø¹ÙÙ„ÙÙŠÙ’Ù‡ÙÙ…Ù’ ÙˆÙÙ„ÙØ§ Ø§Ù„Ø¶Ù‘ÙØ§Ù„Ù‘ÙÙŠÙ†Ù")
]

# Ø§Ø³ØªØ®Ø¯Ù… Ø­Ø§Ù„Ø© session Ù„Ø­ÙØ¸ Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ©
if "verse_index" not in st.session_state:
    st.session_state.verse_index = 0

# Ø²Ø± Ø§Ù„Ø³Ø§Ø¨Ù‚
col1, col2, col3 = st.columns([1, 2, 1])
with col1:
    if st.button("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚") and st.session_state.verse_index > 0:
        st.session_state.verse_index -= 1
with col3:
    if st.button("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸") and st.session_state.verse_index < len(verse_texts) - 1:
        st.session_state.verse_index += 1

# Ø¹Ø±Ø¶ Ø§Ù„Ø¢ÙŠØ©
verse_id, verse_text = verse_texts[st.session_state.verse_index]
st.markdown(f"### Ø§Ù„Ø¢ÙŠØ© {st.session_state.verse_index + 1}:\nğŸ“– {verse_text}")

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª
selected_audio = os.path.join(audio_dir, f"{verse_id}.mp3")
if os.path.exists(selected_audio):
    st.audio(selected_audio, format="audio/mp3")
else:
    st.warning("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ØµÙˆØªÙŠ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ©")

true_text = verse_text

# Ø±ÙØ¹ Ø§Ù„ØµÙˆØª
st.markdown("### ğŸ™ï¸ Ø§Ø±ÙØ¹ ØªÙ„Ø§ÙˆØªÙƒ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ©:")
user_audio = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØªÙƒ (WAV/MP3)", type=["wav", "mp3"])

if user_audio:
    waveform, sr = torchaudio.load(user_audio)
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
    if sr != 16000:
        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)
    inputs = processor(waveform.squeeze(), sampling_rate=16000, return_tensors="pt").to(device)
    with torch.no_grad():
        generated_ids = model.generate(inputs["input_features"], max_new_tokens=200)
    user_transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

    st.subheader("ğŸ“œ ØªÙ„Ø§ÙˆØªÙƒ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
    st.write(user_transcription)

    st.subheader("ğŸ“œ Ø§Ù„Ø¢ÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø©:")
    st.write(true_text)

    # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    diff = list(difflib.ndiff(user_transcription.split(), true_text.split()))
    results = []
    for word in diff:
        token = word[2:].strip()
        if not token or re.fullmatch(r'[\W_]+', token):
            continue
        if word.startswith("+ "):
            results.append({"Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚Ø©": token, "Ø§Ù„Ø­Ø§Ù„Ø©": "ğŸ†• Ø²Ø§Ø¦Ø¯Ø©/Ø®Ø·Ø£", "Ø§Ù„ØªÙØµÙŠÙ„": "Ø²Ø§Ø¯Øª Ø£Ùˆ Ø¨Ù‡Ø§ Ø®Ø·Ø£"})
        elif not word.startswith("- "):
            results.append({"Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ù†Ø·ÙˆÙ‚Ø©": token, "Ø§Ù„Ø­Ø§Ù„Ø©": "âœ… ØµØ­ÙŠØ­", "Ø§Ù„ØªÙØµÙŠÙ„": ""})

    df = pd.DataFrame(results)
    df.index += 1
    df.index.name = "#"

    st.subheader("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ø·Ù‚:")
    st.dataframe(df)

